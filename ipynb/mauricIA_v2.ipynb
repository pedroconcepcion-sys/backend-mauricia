{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c715a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AgentesIA\\MauricIA\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Configurando Agente Inteligente USACH...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Librer√≠as de LangChain y Chroma\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Cargamos variables de entorno (.env)\n",
    "load_dotenv()\n",
    "\n",
    "# --- CONFIGURACI√ìN INICIAL ---\n",
    "CARPETA_DB = \"chroma_db\"\n",
    "MODELO_EMBEDDINGS = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "print(\"‚öôÔ∏è  Configurando Agente Inteligente USACH...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facb8a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuraci√≥n del LLM (Ollama)\n",
    "try:\n",
    "    llm = ChatOllama(\n",
    "        # Valor por defecto seguro\n",
    "        base_url=os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\"),\n",
    "        # Aseg√∫rate de tener el modelo correcto\n",
    "        model=os.getenv(\"OLLAMA_MODEL\", \"llama3\"),\n",
    "        temperature=0.1,  # Temperatura baja para respuestas factuales\n",
    "        num_predict=300,  # Un poco m√°s de espacio para responder\n",
    "    )\n",
    "    print(f\"‚úì Cerebro cargado: {llm.model}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error conectando a Ollama: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Conexi√≥n a la Base de Datos Vectorial (Tu \"Memoria\")\n",
    "# IMPORTANTE: Usamos el MISMO modelo de embeddings que usamos para crear la base de datos\n",
    "print(\"üîå Conectando a la base de conocimiento local...\")\n",
    "\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=MODELO_EMBEDDINGS)\n",
    "\n",
    "if os.path.exists(CARPETA_DB):\n",
    "    vector_db = Chroma(\n",
    "        persist_directory=CARPETA_DB,\n",
    "        embedding_function=embedding_function\n",
    "    )\n",
    "    print(\"‚úì Base de datos ChromaDB conectada.\")\n",
    "else:\n",
    "    print(\"‚úó ERROR CR√çTICO: No encuentro la carpeta 'chroma_db'. Ejecuta primero 'crear_cerebro.py'\")\n",
    "    exit()\n",
    "\n",
    "# Configuraci√≥n del Retriever (Buscador)\n",
    "# k=3 significa que traer√° los 3 fragmentos m√°s relevantes\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "\n",
    "# --- WARM-UP (CALENTAMIENTO DE MOTORES) ---\n",
    "print(\"\\nüî• Iniciando secuencia de calentamiento (para evitar esperas)...\")\n",
    "\n",
    "# 1. Calentar el LLM (Ollama)\n",
    "try:\n",
    "    # Le pedimos algo muy corto para que cargue en memoria sin gastar tiempo generando mucho texto\n",
    "    print(\"   - Cargando modelo LLM en VRAM...\", end=\"\", flush=True)\n",
    "    llm.invoke(\"test\")\n",
    "    print(\" [LISTO]\")\n",
    "except Exception as e:\n",
    "    print(f\" [ERROR LLM: {e}]\")\n",
    "\n",
    "# 2. Calentar el Buscador (Embeddings)\n",
    "# Esto carga el modelo sentence-transformers en memoria\n",
    "try:\n",
    "    print(\"   - Cargando sistema de b√∫squeda sem√°ntica...\", end=\"\", flush=True)\n",
    "    retriever.invoke(\"test\")\n",
    "    print(\" [LISTO]\")\n",
    "except Exception as e:\n",
    "    print(f\" [ERROR RETRIEVER: {e}]\")\n",
    "\n",
    "print(\"‚úì Sistema 100% operativo y listo para recibir usuarios.\\n\")\n",
    "\n",
    "\n",
    "# --- NUEVA FUNCI√ìN PURA (L√≥gica Testable) ---\n",
    "def obtener_respuesta_agente(user_input):\n",
    "    \"\"\"\n",
    "    Recibe el texto del usuario y devuelve la respuesta del agente como string.\n",
    "    No imprime nada, solo procesa.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. DETECTOR DE SALUDOS\n",
    "    palabras_clave = [\n",
    "    \"hola\", \"holi\", \"wena\", \"wenas\", \"buenas\", \"buenos\",\n",
    "    \"buen d√≠a\", \"buen dia\", \"buenas tardes\", \"buenas noches\",\n",
    "    \"saludos\", \"un saludo\", \"saludo\",\n",
    "    \"qu√© tal\", \"que tal\", \"como estas\", \"c√≥mo est√°s\", \"c√≥mo estai\", \"como estai\",\n",
    "    \"hey\", \"hi\", \"hello\", \"yo\", \"sup\"\n",
    "]\n",
    "\n",
    "    es_saludo = any(p in user_input.lower() for p in palabras_clave) and len(user_input.split()) < 6\n",
    "\n",
    "    # Definimos los prompts (Los mismos que ya tienes corregidos)\n",
    "    system_prompt_base = (\n",
    "        \"Eres un experto en admisiones de Postgrado de la USACH llamado asi tal cual MauricIA.\\n\"\n",
    "        \"Atiendes a TODOS los estudiantes. NO te limites a un grupo.\\n\"\n",
    "        \"Tu misi√≥n es extraer informaci√≥n EXACTA del contexto proporcionado. NO inventes ni asumas.\\n\"\n",
    "        \"DEFINICIONES CR√çTICAS:\\n\"\n",
    "        \"- MATR√çCULA: Costo semestral administrativo, valor en el contexto.\\n\"\n",
    "        \"- ARANCEL: Costo anual de estudios, valor en el contexto.\\n\"\n",
    "        \"REGLAS:\\n\"\n",
    "        \"1. Cita TEXTUALMENTE los montos.\\n\"\n",
    "        \"2. Si la info no est√°, di: 'No encuentro ese dato espec√≠fico'.\\n\"\n",
    "    )\n",
    "    \n",
    "    system_prompt_saludo = (\n",
    "        \"Eres MauricIA, el chatbot oficial de porgramas de Postgrados USACH. \"\n",
    "        \"Ayudas a estudiantes chilenos y extranjeros por igual. \"\n",
    "        \"Tu trabajo es saludar amablemente y pedir una consulta acad√©mica. \"\n",
    "    )\n",
    "\n",
    "    full_response = \"\"\n",
    "    messages = []\n",
    "\n",
    "    try:\n",
    "        if es_saludo:\n",
    "            # CAMINO A: Saludo\n",
    "            messages = [\n",
    "                SystemMessage(content=system_prompt_saludo),\n",
    "                HumanMessage(content=user_input)\n",
    "            ]\n",
    "        else:\n",
    "            # CAMINO B: RAG\n",
    "            docs_relacionados = retriever.invoke(user_input)\n",
    "            context_text = \"\\n\\n\".join([d.page_content for d in docs_relacionados])\n",
    "            \n",
    "            messages = [\n",
    "                SystemMessage(content=system_prompt_base),\n",
    "                HumanMessage(content=f\"CONTEXTO:\\n{context_text}\\n\\nPREGUNTA:\\n{user_input}\")\n",
    "            ]\n",
    "\n",
    "        # Invocamos al LLM (sin stream para el test, o acumulando el stream)\n",
    "        # Para tests es mejor invoke directo, pero para mantener tu logica usamos invoke\n",
    "        response = llm.invoke(messages)\n",
    "        return response.content\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error interno: {str(e)}\"\n",
    "\n",
    "# --- LA INTERFAZ DE USUARIO (El Chat) ---\n",
    "def chatbot_streaming():\n",
    "    print(\"\\nüéì === ASISTENTE DE POSTGRADOS USACH ===\")\n",
    "    print(\"Escribe 'salir' para cerrar.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nüßë T√∫: \").strip()\n",
    "        if user_input.lower() in [\"salir\", \"exit\"]: break\n",
    "        if not user_input: continue\n",
    "\n",
    "        print(\"\\nü§ñ Asistente: \", end=\"\", flush=True)\n",
    "        \n",
    "        # Llamamos a la l√≥gica\n",
    "        respuesta = obtener_respuesta_agente(user_input)\n",
    "        \n",
    "        # Simulamos streaming para el usuario (opcional)\n",
    "        print(respuesta) \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot_streaming()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
