import os
import time
import re
import sys
from dotenv import load_dotenv

# --- IMPORTS DE LANGCHAIN, MEMORIA Y VECTORSTORE ---
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
# Componentes clave para el historial de chat
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.output_parsers import StrOutputParser

# --- CORRECCI√ìN AQU√ç: Usamos 'ChatMessageHistory' gen√©rico ---
from langchain_community.chat_message_histories import ChatMessageHistory

# =============================================================================
# 0. CONFIGURACI√ìN INICIAL
# =============================================================================
load_dotenv()

# Configuraci√≥n de Archivos y Modelos
CARPETA_DB = "chroma_db" 
MODELO_EMBEDDINGS = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
SESSION_ID = "sesion_usuario_local"  

"""=== FRONTERA DE PARETO (MEJORES CONFIGURACIONES) ===
ID   | Calidad  | Latencia | Configuraci√≥n
------------------------------------------------------------
1    | 0.67     | 10.80s    | {'k_normal': 7, 'k_dinero': 3, 'max_chars': 12000}
2    | 0.77     | 14.11s    | {'k_normal': 10, 'k_dinero': 3, 'max_chars': 15000}
4    | 0.67     | 10.56s    | {'k_normal': 4, 'k_dinero': 5, 'max_chars': 10000}
5    | 0.63     | 10.10s    | {'k_normal': 7, 'k_dinero': 3, 'max_chars': 6000}
6    | 0.84     | 14.63s    | {'k_normal': 4, 'k_dinero': 4, 'max_chars': 12000}--------------Joya de la Corona
9    | 0.77     | 13.42s    | {'k_normal': 7, 'k_dinero': 4, 'max_chars': 7000} --------------M√°s velocidad
11   | 0.85     | 17.39s    | {'k_normal': 10, 'k_dinero': 5, 'max_chars': 16000}"""

MAX_CONTEXT_CHARS = 12000  
K_NORMAL = 4              
K_DINERO = 10 

print("‚öôÔ∏è  Inicializando MauricIA v3 (Corregido)...")

# =============================================================================
# 1. CARGA DE MODELOS (CEREBRO Y MEMORIA)
# =============================================================================

# üéõÔ∏è INTERRUPTOR MAESTRO: ¬øUsar PC Local o Nube Potente?
# True  = Gratis, Llama 3.1, Privacidad total.
# False = Pago, GPT-4o, M√°xima inteligencia.
MODO_LOCAL = False 

try:
    if MODO_LOCAL:
        # A) LLM LOCAL (Llama 3.1 via Ollama)
        print("   - üè† Modo Local. Conectando con Ollama...", end=" ")
        llm = ChatOllama(
            base_url=os.getenv("OLLAMA_BASE_URL"),
            model=os.getenv("OLLAMA_MODEL"), 
            temperature=0.0,
            num_predict=300,
        )
        print(f"‚úÖ Listo: {llm.model}")

    else:
        # B) LLM NUBE (GPT-4o via OpenAI)
        print("   - ‚òÅÔ∏è Modo Nube. Conectando con OpenAI...", end=" ")
        
        # Verificaci√≥n de seguridad para no gastar cr√©ditos por error
        if not os.getenv("GITHUB_TOKEN"):
            raise ValueError("‚ùå ERROR: MODO_LOCAL es False, pero no encontr√© la OPENAI_API_KEY en .env")

        llm = ChatOpenAI(
            base_url=os.getenv("OPENAI_BASE_URL"),
            model=os.getenv("MODEL_NAME"),
            api_key=os.getenv("GITHUB_TOKEN"),
            temperature=0.0,
            max_tokens=300
            
        )
        print(f"‚úÖ Listo: {llm.model_name}")

except Exception as e:
    print(f"\n‚ùå Error cr√≠tico cargando el modelo LLM: {e}")
    sys.exit(1)

# B) EMBEDDINGS & VECTOR STORE (ChromaDB)
print("   - Cargando Base de Conocimiento...", end=" ")
embedding_function = HuggingFaceEmbeddings(model_name=MODELO_EMBEDDINGS)

if os.path.exists(CARPETA_DB):
    vector_db = Chroma(
        persist_directory=CARPETA_DB,
        embedding_function=embedding_function
    )
    print("‚úÖ ChromaDB conectado.")
else:
    print("\n‚ùå ERROR: No existe la carpeta 'chroma_db'.")
    print("‚ö†Ô∏è  SOLUCI√ìN: Ejecuta primero 'python crear_cerebro_refinado_v5.py'")
    sys.exit(1)

# =============================================================================
# 2. PROMPT DEL SISTEMA (LA L√ìGICA DE NEGOCIO)
# =============================================================================
SYSTEM_PROMPT_V3 = (
    "Eres MauricIA, la asistente oficial de Postgrados USACH.\n"
    "Tus instrucciones son INVIOLABLES. Responde bas√°ndote en el CONTEXTO y el HISTORIAL.\n"
    "\n"
    "üß† PROTOCOLO DE RAZONAMIENTO (NO IMPRIMIR):\n"
    "1. ANALIZA EL HISTORIAL MENTALMENTE: Revisa si el usuario ya mencion√≥ un programa (ej: 'Mag√≠ster en Inform√°tica').\n"
    "   - Si pregunta \"¬øCu√°nto cuesta?\" y antes hablaron del Mag√≠ster, asume que es sobre ese.\n"
    "2. DETECCI√ìN DE AMBIG√úEDAD:\n"
    "   - Si el usuario pregunta por un dato gen√©rico y NO sabes el programa:\n"
    "   - üõë DETENTE Y PREGUNTA: \"¬øA cu√°l programa te refieres? Tengo informaci√≥n de Doctorados, Mag√≠sters, etc.\"\n"
    "   - Si el contexto trae info de DOS programas, difer√©ncialos: \"Para el Doctorado es X, para el Mag√≠ster es Y\".\n"
    "‚õî PROHIBICIONES DE FORMATO (CR√çTICO):\n"
    "   - NO uses etiquetas como 'Respuesta:', 'Formato:', 'An√°lisis:', 'Paso 1:'.\n"
    "   - NO expliques tu comportamiento (ej: 'La respuesta se enfoca en...').\n"
    "   - NO imprimas tu pensamiento interno.\n"
    "   - Solo entrega el mensaje final para el usuario de forma natural.\n"
    "\n"
    "üö® REGLAS DE SEGURIDAD:\n"
    "- ‚õî NO ACAD√âMICO: Si piden recetas, gym, piscina o clima -> \"No tengo informaci√≥n sobre servicios no acad√©micos.\"\n"
    "- ‚úÖ INFORMACI√ìN V√ÅLIDA: Costos, Mallas, Becas, Requisitos y CONTACTO (Nombres de secretarias, coordinadores, correos).\n"
    "- ‚úÖ Los programas tanto de mag√≠ster como de doctorado no son dedicaci√≥n exlusiva, se puede trabajar mientras se estudia a la vez."
    "- üìù Si preguntan: Profesores/Docentes/Acad√©micos del claustro de cualquier programa, responde que: estar√° pronto en el contexto, a√∫n no lo hemos actualizado esa informaci√≥n"
    "- üìù Si preguntan: Que nota m√≠nima de pregrado para ser aceptado en algun programa? - respondes: la nota no influye en la aceptacion, contactar a CONTACTO del programa"
    "- üìù Si preguntan: L√≠neas/Lineas de investigaci√≥n de los programas: responde que: estar√° pronto en el contexto, a√∫n no hemos actualizado esa informaci√≥n"
    "- üìù Si preguntan: Hay convenios de co-tutela con universidades extranjeras? , respondes que si, m√°s informaci√≥n en el CONTACTO"
    "- üìù Si preguntan: Si estudi√© una carrera de otra √°rea distinta, puedo postular al doctorado o magister: respondes que si"
    "- üìß CONTACTO: Si preguntan por la secretaria/o, busca en la secci√≥n de 'CONTACTO' del texto y entrega el nombre y correo si aparece.\n"
    "üí∞ REGLAS FINANCIERAS (ESTRICTO):\n"
    "- MATR√çCULA (~$167.000, semestral) != ARANCEL (Millones, anual).\n"
    "- üö´ PROHIBIDO MULTIPLICAR la matr√≠cula por 2. Entrega el valor semestral tal cual.\n" 
    "- Busca el valor exacto en el texto para el programa espec√≠fico.\n"
    "- PROHIBIDO MULTIPLICAR o sumar.\n"
    "\n"
    "üìù FORMATO:\n"
    "- Respuesta directa, c√°lida y profesional.\n"
    "- Usa VI√ëETAS para listas (becas, requisitos, etc...).\n"
    "- üìé LINKS: Si el texto dice 'PUEDES DESCARGAR EL PDF AQU√ç', entr√©galo al final con emoji üì•."
)

# Respuestas r√°pidas
RESP_NO_ACADEMICO = "No tengo informaci√≥n sobre servicios no acad√©micos, solo sobre postgrados."
RESP_BLOQUEO = "Lo siento, solo puedo responder consultas sobre Postgrados USACH."

# =============================================================================
# 3. FILTROS Y SEGURIDAD (CAPA PYTHON)
# =============================================================================
INYECCION_PROHIBIDA = [
    "ignora", "ignore", "olvida", "forget", "system prompt", "instrucciones",
    "revela", "jailbreak", "dan", "modo desarrollador"
]
NO_ACADEMICO_KW = [
    "receta", "cocina", "pizza", "sushi", "chiste", "clima", 
    "piscina", "gimnasio", "gym", "casino", "men√∫"
]
SALUDOS_KW = {
    "hola", "holi", "buenas", "buenos", "dias", "tardes", "noches",
    "saludos", "hey", "hi", "que", "tal", "mauricia"
}
KW_DINERO = ("cuanto", "precio", "valor", "costo", "sale", "arancel", "matricula")

_re_inyeccion = re.compile("|".join(re.escape(x) for x in INYECCION_PROHIBIDA), re.IGNORECASE)
_re_noacad = re.compile("|".join(re.escape(x) for x in NO_ACADEMICO_KW), re.IGNORECASE)

def es_saludo_puro(user_input: str) -> bool:
    t = re.sub(r'[^\w\s]', '', (user_input or "").lower().strip())
    words = t.split()
    return len(words) < 6 and any(w in SALUDOS_KW for w in words)

def es_consulta_dinero(user_input: str) -> bool:
    return any(k in (user_input or "").lower() for k in KW_DINERO)

def detectar_filtro_programa(consulta: str):
    """
    Analiza la consulta y crea un filtro de metadatos para ChromaDB.
    Esto impide f√≠sicamente que se mezclen peras con manzanas.
    """
    consulta = consulta.lower()
    
    # Palabras clave que identifican inequ√≠vocamente al programa
    es_doctorado = "doctorado" in consulta or "doctor" in consulta or "phd" in consulta 
    es_magister = "magister" in consulta or "mag√≠ster" in consulta or "mgi" in consulta or "master" in consulta or "m√°ster" in consulta
    es_diplomado = "diplomado" in consulta

    # L√≥gica de exclusi√≥n:
    # Si pregunta por Doctorado, filtramos para que la fuente contenga "doctorado"
    if es_doctorado and not es_magister:
        print("   [üîç Filtro Activo]: Buscando SOLO en documentos de DOCTORADO.")
        return {"source": {"$contains": "doctorado"}} # Sintaxis de ChromaDB
    
    elif es_magister and not es_doctorado:
        print("   [üîç Filtro Activo]: Buscando SOLO en documentos de MAG√çSTER.")
        return {"source": {"$contains": "magister"}} # Asume que tus archivos se llaman 'magister_algo.md'
    
    elif es_diplomado:
        return {"source": {"$contains": "diplomado"}}

    # Si menciona ambos o ninguno (ej: "¬øQu√© postgrados tienen?"), no filtramos
    print("   [üîç Filtro Inactivo]: Buscando en TODO el cerebro.")
    return None

# =============================================================================
# 4. CONFIGURACI√ìN DE LA CADENA CON MEMORIA (LANGCHAIN)
# =============================================================================

# A) Template del Chat
qa_prompt = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_PROMPT_V3),
    MessagesPlaceholder(variable_name="chat_history"), 
    ("human", "CONTEXTO RECUPERADO:\n{context}\n\nPREGUNTA DEL USUARIO:\n{input}")
])

# B) Cadena Base
chain = qa_prompt | llm | StrOutputParser()

# C) Almac√©n de Sesiones (Memoria RAM)
store = {}

def get_session_history(session_id: str):
    if session_id not in store:
        # --- CORRECCI√ìN AQU√ç: Usamos ChatMessageHistory ---
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# D) Cadena Final con Historial Autom√°tico
conversational_rag_chain = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

# =============================================================================
# 5. L√ìGICA DEL AGENTE (RAG + MEMORIA)
# =============================================================================
def obtener_respuesta_agente(user_input: str) -> str:
    user_input = (user_input or "").strip()
    if not user_input: return "..."

    # --- FASE 1: Filtros de Seguridad ---
    if _re_inyeccion.search(user_input): return RESP_BLOQUEO
    if _re_noacad.search(user_input): return RESP_NO_ACADEMICO

    # --- FASE 2: Saludo R√°pido ---
    if es_saludo_puro(user_input):
        return "¬°Hola! Soy MauricIA, tu asistente de Postgrados USACH. ¬øSobre qu√© programa te gustar√≠a informarte hoy?"

    # --- FASE 3: RAG (Retrieval Augmented Generation) ---
    try:
        k_val = K_DINERO if es_consulta_dinero(user_input) else K_NORMAL
        query_search = user_input
        if es_consulta_dinero(user_input):
            query_search += " arancel matr√≠cula costo valor anual semestral pesos matricula"
             

        docs = vector_db.similarity_search(query_search, k=k_val)
        
        contexto_str = "\n\n".join([d.page_content for d in docs])
        if len(contexto_str) > MAX_CONTEXT_CHARS:
            contexto_str = contexto_str[:MAX_CONTEXT_CHARS]

        if not docs:
            contexto_str = "No se encontr√≥ informaci√≥n espec√≠fica en la base de datos."

        respuesta = conversational_rag_chain.invoke(
            {"input": user_input, "context": contexto_str},
            config={"configurable": {"session_id": SESSION_ID}}
        )
        return respuesta

    except Exception as e:
        return f"‚ö†Ô∏è Error interno del sistema: {str(e)}"

# =============================================================================
# 6. INTERFAZ DE USUARIO (CLI)
# =============================================================================
def chatbot_streaming():
    print("\nüéì === ASISTENTE DE POSTGRADOS USACH (MauricIA v3) ===")
    print("   (Escribe 'salir' para cerrar)\n")

    print("üî• Calentando motores...", end="", flush=True)
    try:
        vector_db.similarity_search("test", k=1)
        print(" Listo.")
    except:
        print(" (Advertencia: Primera consulta puede ser lenta)")

    while True:
        try:
            user_input = input("\nüßë T√∫: ").strip()
        except EOFError:
            break
            
        if user_input.lower() in ["salir", "exit", "chao"]:
            print("\nü§ñ MauricIA: ¬°Mucho √©xito en tu postulaci√≥n! Hasta luego.")
            break
        
        if not user_input: continue

        print("\nü§ñ MauricIA: ", end="", flush=True)
        respuesta = obtener_respuesta_agente(user_input)

        for char in respuesta:
            print(char, end="", flush=True)
            time.sleep(0.04)
        print()

if __name__ == "__main__":
    chatbot_streaming()